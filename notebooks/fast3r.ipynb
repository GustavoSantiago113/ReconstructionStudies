{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed43650e",
   "metadata": {},
   "source": [
    "# Fast3r implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "408c7b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\Documents\\ProjetosDiversao\\ReconstructionStudies\\.venv\\Lib\\site-packages\\pl_bolts\\__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "c:\\Users\\gusta\\Documents\\ProjetosDiversao\\ReconstructionStudies\\.venv\\Lib\\site-packages\\lightning_fabric\\__init__.py:29: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "c:\\Users\\gusta\\Documents\\ProjetosDiversao\\ReconstructionStudies\\.venv\\Lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "c:\\Users\\gusta\\Documents\\ProjetosDiversao\\ReconstructionStudies\\.venv\\Lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "c:\\Users\\gusta\\Documents\\ProjetosDiversao\\ReconstructionStudies\\.venv\\Lib\\site-packages\\pl_bolts\\losses\\self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/!\\ module trimesh is not installed, cannot visualize results /!\\\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fast3r.dust3r.utils.image import load_images\n",
    "from fast3r.dust3r.inference_multiview import inference\n",
    "from fast3r.models.fast3r import Fast3R\n",
    "from fast3r.models.multiview_dust3r_module import MultiViewDUSt3RLitModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "533b8732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "model = Fast3R.from_pretrained(\"jedyang97/Fast3R_ViT_Large_512\")  # If you have networking issues, try pre-download the HF checkpoint dir and change the path here to a local directory\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1992ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading images from ../images/\n",
      " - adding stop_01_20251128_141444.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_02_20251128_141452.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_03_20251128_141501.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_04_20251128_141510.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_05_20251128_141518.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_06_20251128_141527.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_07_20251128_141536.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_08_20251128_141544.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_09_20251128_141553.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_10_20251128_141602.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_11_20251128_141611.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_12_20251128_141620.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_13_20251128_141628.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_14_20251128_141637.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_15_20251128_141646.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_16_20251128_141655.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_17_20251128_141703.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_18_20251128_141712.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_19_20251128_141721.jpg with resolution 3000x4000 --> 384x512\n",
      " - adding stop_20_20251128_141730.jpg with resolution 3000x4000 --> 384x512\n",
      " (Found 20 images)\n"
     ]
    }
   ],
   "source": [
    "# Create a lightweight lightning module wrapper for the model.\n",
    "# This provides functions to estimate camera poses, evaluate 3D reconstruction, etc.\n",
    "lit_module = MultiViewDUSt3RLitModule.load_for_inference(model)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "lit_module.eval()\n",
    "\n",
    "# --- Load Images ---\n",
    "# Provide a list of image file paths. Images can come from different cameras and aspect ratios.\n",
    "filelist = \"../images/\"  # Change this to your image directory\n",
    "images = load_images(filelist, size=512, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b1e1fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Inference with model on 20 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\Documents\\ProjetosDiversao\\ReconstructionStudies\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:287: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n",
      "c:\\Users\\gusta\\Documents\\ProjetosDiversao\\ReconstructionStudies\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something is wrong with the encoder, it took: 438.4850859642029\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Run Inference ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# The inference function returns a dictionary with predictions and view information.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m output_dict, profiling_info = \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or use torch.bfloat16 if supported\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprofiling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusta\\Documents\\ProjetosDiversao\\ReconstructionStudies\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\ProjetosDiversao\\ReconstructionStudies\\fast3r\\fast3r\\dust3r\\inference_multiview.py:82\u001b[39m, in \u001b[36minference\u001b[39m\u001b[34m(multiple_views_in_one_sample, model, device, dtype, verbose, profiling)\u001b[39m\n\u001b[32m     79\u001b[39m     batch_size = \u001b[32m1\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Get the result from loss_of_one_batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m res = \u001b[43mloss_of_one_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollate_with_cat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmultiple_views_in_one_sample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprofiling\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Extract profiling_info before to_cpu if it exists\u001b[39;00m\n\u001b[32m     87\u001b[39m profiling_info = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\ProjetosDiversao\\ReconstructionStudies\\fast3r\\fast3r\\dust3r\\inference_multiview.py:56\u001b[39m, in \u001b[36mloss_of_one_batch\u001b[39m\u001b[34m(batch, model, criterion, device, precision, symmetrize_batch, use_amp, ret, profiling)\u001b[39m\n\u001b[32m     54\u001b[39m     preds, profiling_info = model(views, profiling=profiling)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviews\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprofiling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# loss is supposed to be symmetric\u001b[39;00m\n\u001b[32m     59\u001b[39m loss = (\n\u001b[32m     60\u001b[39m     criterion(views, preds) \u001b[38;5;28;01mif\u001b[39;00m criterion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     61\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusta\\Documents\\ProjetosDiversao\\ReconstructionStudies\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusta\\Documents\\ProjetosDiversao\\ReconstructionStudies\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\ProjetosDiversao\\ReconstructionStudies\\fast3r\\fast3r\\models\\fast3r.py:328\u001b[39m, in \u001b[36mFast3R.forward\u001b[39m\u001b[34m(self, views, profiling)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;66;03m# print the image and true_shape\u001b[39;00m\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m view_idx, view \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(views):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mview_idx: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m, view name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mview_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m, image content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview[\u001b[33m'\u001b[39m\u001b[33mimg\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m, true_shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview[\u001b[33m'\u001b[39m\u001b[33mtrue_shape\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# Create image IDs for each patch\u001b[39;00m\n\u001b[32m    331\u001b[39m pos_emb_start_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\ProjetosDiversao\\ReconstructionStudies\\fast3r\\fast3r\\dust3r\\datasets\\base\\base_stereo_view_dataset.py:237\u001b[39m, in \u001b[36mview_name\u001b[39m\u001b[34m(view, batch_index)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msel\u001b[39m(x):\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x[batch_index] \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m db = sel(\u001b[43mview\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m    238\u001b[39m label = sel(view[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    239\u001b[39m instance = sel(view[\u001b[33m\"\u001b[39m\u001b[33minstance\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mKeyError\u001b[39m: 'dataset'"
     ]
    }
   ],
   "source": [
    "# --- Run Inference ---\n",
    "# The inference function returns a dictionary with predictions and view information.\n",
    "output_dict, profiling_info = inference(\n",
    "    images,\n",
    "    model,\n",
    "    device,\n",
    "    dtype=torch.bfloat16,  # or use torch.bfloat16 if supported\n",
    "    verbose=True,\n",
    "    profiling=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dc14a98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Estimate Camera Poses ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This step estimates the camera-to-world (c2w) poses for each view using PnP.\u001b[39;00m\n\u001b[32m      3\u001b[39m poses_c2w_batch, estimated_focals = MultiViewDUSt3RLitModule.estimate_camera_poses(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43moutput_dict\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mpreds\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      5\u001b[39m     niter_PnP=\u001b[32m100\u001b[39m,\n\u001b[32m      6\u001b[39m     focal_length_estimation_method=\u001b[33m'\u001b[39m\u001b[33mfirst_view_from_global_head\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# poses_c2w_batch is a list; the first element contains the estimated poses for each view.\u001b[39;00m\n\u001b[32m      9\u001b[39m camera_poses = poses_c2w_batch[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'output_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Estimate Camera Poses ---\n",
    "# This step estimates the camera-to-world (c2w) poses for each view using PnP.\n",
    "poses_c2w_batch, estimated_focals = MultiViewDUSt3RLitModule.estimate_camera_poses(\n",
    "    output_dict['preds'],\n",
    "    niter_PnP=100,\n",
    "    focal_length_estimation_method='first_view_from_global_head'\n",
    ")\n",
    "# poses_c2w_batch is a list; the first element contains the estimated poses for each view.\n",
    "camera_poses = poses_c2w_batch[0]\n",
    "\n",
    "# Print camera poses for all views.\n",
    "for view_idx, pose in enumerate(camera_poses):\n",
    "    print(f\"Camera Pose for view {view_idx}:\")\n",
    "    print(pose.shape)  # np.array of shape (4, 4), the camera-to-world transformation matrix\n",
    "\n",
    "# --- Extract 3D Point Clouds for Each View ---\n",
    "# Each element in output_dict['preds'] corresponds to a view's point map.\n",
    "for view_idx, pred in enumerate(output_dict['preds']):\n",
    "    point_cloud = pred['pts3d_in_other_view'].cpu().numpy()\n",
    "    print(f\"Point Cloud Shape for view {view_idx}: {point_cloud.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
