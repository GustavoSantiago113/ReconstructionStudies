{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9befd1a",
   "metadata": {},
   "source": [
    "# Segmenting Images using YOLO model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f4a12",
   "metadata": {},
   "source": [
    "## Training Semantic Segmentation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505a7b5",
   "metadata": {},
   "source": [
    "### Agumenting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd5e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageEnhance\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def augment_image(img):\n",
    "    augmented = []\n",
    "    # 1. Horizontal flip\n",
    "    augmented.append(img.transpose(Image.FLIP_LEFT_RIGHT))\n",
    "    # 2. Vertical flip\n",
    "    augmented.append(img.transpose(Image.FLIP_TOP_BOTTOM))\n",
    "    # 3. Rotate 90 degrees\n",
    "    augmented.append(img.rotate(90, expand=True))\n",
    "    # 4. Change saturation\n",
    "    enhancer = ImageEnhance.Color(img)\n",
    "    augmented.append(enhancer.enhance(random.uniform(0.1, 2)))\n",
    "    # 5. Change brightness\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    augmented.append(enhancer.enhance(random.uniform(0.1, 2)))\n",
    "    return augmented\n",
    "\n",
    "def visualize_augmentation(img):\n",
    "    augmented = augment_image(img)\n",
    "    fig, axes = plt.subplots(1, len(augmented) + 1, figsize=(15, 4))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    for i, aug_img in enumerate(augmented):\n",
    "        axes[i+1].imshow(aug_img)\n",
    "        axes[i+1].set_title(f'Aug {i+1}')\n",
    "        axes[i+1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def augment_and_save(base_dir, visualize=False):\n",
    "    #for split in ['train', 'valid']:\n",
    "    for split in ['valid']:\n",
    "        for subfolder in ['images', 'masks']:\n",
    "            folder = os.path.join(base_dir, split, subfolder)\n",
    "            for fname in os.listdir(folder):\n",
    "                if not fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "                    continue\n",
    "                fpath = os.path.join(folder, fname)\n",
    "                with Image.open(fpath) as img:\n",
    "                    # For masks, only apply geometric transforms (no color/brightness)\n",
    "                    if subfolder == 'masks':\n",
    "                        # For masks, only apply geometric transforms (flips and 90-degree rotation), \n",
    "                        # for Aug 4 and 5 just copy the original mask (no-op)\n",
    "                        augmented = [\n",
    "                            img.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "                            img.transpose(Image.FLIP_TOP_BOTTOM),\n",
    "                            img.rotate(90, expand=True),\n",
    "                            img.copy(),  # Aug 4: just copy the original mask\n",
    "                            img.copy()   # Aug 5: just copy the original mask\n",
    "                        ]\n",
    "                        if visualize:\n",
    "                            fig, axes = plt.subplots(1, len(augmented) + 1, figsize=(15, 4))\n",
    "                            axes[0].imshow(img)\n",
    "                            axes[0].set_title('Original Mask')\n",
    "                            axes[0].axis('off')\n",
    "                            for i, aug_img in enumerate(augmented):\n",
    "                                axes[i+1].imshow(aug_img)\n",
    "                                axes[i+1].set_title(f'Aug {i+1}')\n",
    "                                axes[i+1].axis('off')\n",
    "                            plt.tight_layout()\n",
    "                            plt.show()\n",
    "                            # Only visualize the first mask and return\n",
    "                            return\n",
    "                    else:\n",
    "                        augmented = augment_image(img)\n",
    "                        if visualize:\n",
    "                            visualize_augmentation(img)\n",
    "                            # Only visualize the first image and return\n",
    "                            return\n",
    "                    for i, aug_img in enumerate(augmented):\n",
    "                        name, ext = os.path.splitext(fname)\n",
    "                        aug_fname = f\"{name}_aug{i+1}{ext}\"\n",
    "                        aug_img.save(os.path.join(folder, aug_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad94a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_and_save(base_dir='../images_train', visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5d399",
   "metadata": {},
   "source": [
    "### Creating Ground Truth Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42922309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_masks_from_yolov8(images_folder, labels_folder, masks_folder, img_ext='jpg', label_ext='txt', show_example=True):\n",
    "    os.makedirs(masks_folder, exist_ok=True)\n",
    "    image_files = [f for f in os.listdir(images_folder) if f.endswith(f'.{img_ext}')]\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(images_folder, img_file)\n",
    "        label_file = img_file.replace(f'.{img_ext}', f'.{label_ext}')\n",
    "        label_path = os.path.join(labels_folder, label_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        h, w = img.shape[:2]\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 2:\n",
    "                        continue\n",
    "                    # Yolov8 format: class_id x_center y_center width height [polygon points...]\n",
    "                    if len(parts) > 5:\n",
    "                        # polygon mask (segmentation)\n",
    "                        pts = np.array(parts[1:], dtype=float).reshape(-1, 2)\n",
    "                        pts[:, 0] *= w\n",
    "                        pts[:, 1] *= h\n",
    "                        pts = pts.astype(np.int32)\n",
    "                        cv2.fillPoly(mask, [pts], 255)\n",
    "                    else:\n",
    "                        # bbox mask\n",
    "                        _, x, y, bw, bh = map(float, parts[:5])\n",
    "                        x1 = int((x - bw/2) * w)\n",
    "                        y1 = int((y - bh/2) * h)\n",
    "                        x2 = int((x + bw/2) * w)\n",
    "                        y2 = int((y + bh/2) * h)\n",
    "                        cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)\n",
    "        # Show example\n",
    "        if show_example:\n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.title('Image')\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.title('Mask')\n",
    "            plt.imshow(mask, cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            break\n",
    "        else:\n",
    "            mask_path = os.path.join(masks_folder, img_file.replace(f'.{img_ext}', '.png'))\n",
    "            cv2.imwrite(mask_path, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1245be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_masks_from_yolov8('../images_train/valid/images', '../images_train/valid/labels', '../images_train/valid/masks', show_example=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbec426",
   "metadata": {},
   "source": [
    "### Loading the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bd7aece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train size: 4788 | Test size: 546\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "dataset_path = \"../images_train/\"\n",
    "\n",
    "# --- Custom Dataset for Segmentation ---\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.img_dir = os.path.join(root_dir, \"train/images\" if train else \"valid/images\")\n",
    "        self.mask_dir = os.path.join(root_dir, \"train/masks\" if train else \"valid/masks\")\n",
    "\n",
    "        self.image_files = sorted([\n",
    "            f for f in os.listdir(self.img_dir)\n",
    "            if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ])\n",
    "        self.mask_files = sorted([\n",
    "            f for f in os.listdir(self.mask_dir)\n",
    "            if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ])\n",
    "\n",
    "        assert len(self.image_files) == len(self.mask_files), \\\n",
    "            f\"Image/Mask count mismatch: {len(self.image_files)} vs {len(self.mask_files)}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # grayscale mask\n",
    "\n",
    "        # --- To Tensor ---\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "        # Mask â†’ tensor (no normalization)\n",
    "        mask = torch.from_numpy(np.array(mask)).float().unsqueeze(0)  # [1, H, W]\n",
    "        mask = (mask > 0).float()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# --- Datasets & Loaders ---\n",
    "train_dataset = SegmentationDataset(dataset_path, train=True)\n",
    "test_dataset = SegmentationDataset(dataset_path, train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"âœ… Train size: {len(train_dataset)} | Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5c4ba",
   "metadata": {},
   "source": [
    "### Importing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54304f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f0d0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.models.uNet import UNet\n",
    "\n",
    "channels = [32, 64, 128, 256, 512]\n",
    "model = UNet(in_channels=3, out_channels=1, channels=channels, bilinear=True, use_batchnorm=True)\n",
    "model.to(device)\n",
    "modelName = \"U-NET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c814577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.models.deeplabv3p import DeepLabV3Plus\n",
    "\n",
    "model = DeepLabV3Plus(num_classes=1, output_stride=16, backbone_width_mult=1.0).to(device)\n",
    "model.to(device)\n",
    "modelName = \"DeepLabV3Plus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb70df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gnoceras\\Documents\\GustavoPersonal\\ReconstructionStudies\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\gnoceras\\Documents\\GustavoPersonal\\ReconstructionStudies\\.venv\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.models.SegFormer import segformer\n",
    "\n",
    "model = segformer(in_channels = 3, num_classes = 1)\n",
    "model.to(device)\n",
    "modelName = \"SegFormer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea68d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.models.SegNet import segnet\n",
    "\n",
    "model = segnet(in_channels=3, num_classes=1, pretrained=False)\n",
    "model.to(device)\n",
    "modelName = \"SegNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff52e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gnoceras\\Documents\\GustavoPersonal\\ReconstructionStudies\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.models.maskFormer import MaskFormer\n",
    "from utils.models.resnet101 import resnet101_backbone\n",
    "\n",
    "resnet101 = resnet101_backbone()\n",
    "model = MaskFormer(\n",
    "    backbone=resnet101, \n",
    "    num_classes=1, \n",
    "    num_queries=5,          # Reduced from 10\n",
    "    embed_dim=64,           # Reduced from 128  \n",
    "    transformer_layers=1,   # Reduced from 2\n",
    "    transformer_heads=2,    # Reduced from 4\n",
    "    transformer_ffn_dim=256, # Reduced from 512\n",
    "    return_binary=True\n",
    ").to(device)\n",
    "modelName = \"MaskFormer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf93df",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6940c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf16a807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoints will be saved to: ../models/MaskFormer_seg.pt\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "EPOCHS = 200\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf') \n",
    "patience_counter = 0\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "checkpoint_path = os.path.join(\"../models/\", modelName + \"_seg.pt\")\n",
    "print(f\"Model checkpoints will be saved to: {checkpoint_path}\")\n",
    "\n",
    "# --- Tracking ---\n",
    "train_losses, val_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c563fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\") as tepoch:\n",
    "        for inputs, labels in tepoch:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(inputs)\n",
    "                if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                    print(\"NaN or Inf in model outputs!\")\n",
    "                if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "                    print(\"NaN or Inf in labels!\")\n",
    "                if labels.sum() == 0:\n",
    "                    print(\"Skipping batch with all empty masks\")\n",
    "                    continue\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=running_loss / (len(tepoch)))\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\", unit=\"batch\") as vepoch:\n",
    "            for inputs, labels in vepoch:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    outputs = model(inputs)\n",
    "                    if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                        print(\"NaN or Inf in model outputs!\")\n",
    "                    if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "                        print(\"NaN or Inf in labels!\")\n",
    "                    if labels.sum() == 0:\n",
    "                        print(\"Skipping batch with all empty masks\")\n",
    "                        continue\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                    preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.numel()\n",
    "                val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "                val_loss += loss.item()\n",
    "                vepoch.set_postfix(loss=val_loss / (len(vepoch)))\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Val Loss: {avg_val_loss:.4f} - Val Acc: {val_acc:.4f}%\")\n",
    "\n",
    "    # --- Scheduler step (use validation loss) ---\n",
    "    lr_scheduler.step(avg_val_loss)\n",
    "\n",
    "    # --- Checkpointing ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"âœ… Model improved (Val Loss={avg_val_loss:.3f}), saved to {checkpoint_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"ðŸ”´ No improvement, patience counter: {patience_counter}\")\n",
    "\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(\"ðŸ›‘ Early stopping triggered!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot losses ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Segmentation Training & Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8296a75",
   "metadata": {},
   "source": [
    "### Inference on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c14495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Pick 5 random indices from the test dataset\n",
    "indices = random.sample(range(len(test_dataset)), 5)\n",
    "\n",
    "fig, axs = plt.subplots(5, 3, figsize=(12, 18))\n",
    "for i, idx in enumerate(indices):\n",
    "    img, mask = test_dataset[idx]\n",
    "    img_input = img.unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred_logits = model(img_input)\n",
    "            pred_mask = (torch.sigmoid(pred_logits) > 0.5).float().cpu().squeeze().numpy()\n",
    "    axs[i, 0].imshow(img.permute(1, 2, 0).cpu() * 0.5 + 0.5)\n",
    "    axs[i, 0].set_title(\"Input Image\")\n",
    "    axs[i, 1].imshow(mask.squeeze().cpu(), cmap='gray')\n",
    "    axs[i, 1].set_title(\"Ground Truth Mask\")\n",
    "    axs[i, 2].imshow(pred_mask, cmap='gray')\n",
    "    axs[i, 2].set_title(\"Predicted Mask\")\n",
    "    for j in range(3):\n",
    "        axs[i, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c75814",
   "metadata": {},
   "source": [
    "### Choosing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ead42227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class SegEvalDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, image_files):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = image_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        mask_name = os.path.splitext(img_name)[0] + \".png\"\n",
    "        image = cv2.imread(os.path.join(self.image_dir, img_name))\n",
    "        mask = cv2.imread(os.path.join(self.mask_dir, mask_name), cv2.IMREAD_GRAYSCALE)\n",
    "        if mask.ndim == 3:\n",
    "            mask = mask[..., 0]\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        mask = (mask > 0).astype(np.float32)\n",
    "        mask = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "        return image, mask, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c109e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Utility functions\n",
    "# ======================\n",
    "\n",
    "def iou_score(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-7)\n",
    "\n",
    "def load_model(model_name, device):\n",
    "    if model_name == \"U-NET\":\n",
    "        channels = [32, 64, 128, 256, 512]\n",
    "        model = UNet(in_channels=3, out_channels=1, channels=channels, bilinear=True, use_batchnorm=True).to(device)\n",
    "    elif model_name == \"SegFormer\":\n",
    "        model = segformer(in_channels = 3, num_classes = 1).to(device)\n",
    "    elif model_name == \"DeepLabV3Plus\":\n",
    "        model = DeepLabV3Plus(num_classes=1, output_stride=16, backbone_width_mult=1.0).to(device)\n",
    "    elif model_name == \"SegNet\":\n",
    "        model = segnet(in_channels=3, num_classes=1, pretrained=False).to(device)\n",
    "    elif model_name == \"MaskFormer\":\n",
    "        resnet101 = resnet101_backbone()\n",
    "        model = MaskFormer(\n",
    "            backbone=resnet101, \n",
    "                num_classes=1, \n",
    "                num_queries=5,\n",
    "                embed_dim=64, \n",
    "                transformer_layers=1,\n",
    "                transformer_heads=2,\n",
    "                transformer_ffn_dim=256,\n",
    "                return_binary=True\n",
    "            ).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "    model.load_state_dict(torch.load(f'../models/{model_name}_seg.pt', map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict_mask(model, image, device):\n",
    "    img_input = image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            pred_logits = model(img_input)\n",
    "            pred_mask = (torch.sigmoid(pred_logits) > 0.5).float().cpu().squeeze().numpy()\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853c15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "from utils.models.maskFormer import MaskFormer\n",
    "from utils.models.resnet101 import resnet101_backbone\n",
    "from utils.models.uNet import UNet\n",
    "from utils.models.SegNet import segnet\n",
    "from utils.models.SegFormer import segformer\n",
    "from utils.models.deeplabv3p import DeepLabV3Plus\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = \"../images_train/\"\n",
    "models = [\"DeepLabV3Plus\", \"SegFormer\", \"SegNet\", \"U-NET\", \"MaskFormer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10548cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DeepLabV3Plus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [00:12<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SegFormer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [00:10<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SegNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [00:16<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating U-NET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [00:14<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MaskFormer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gnoceras\\Documents\\GustavoPersonal\\ReconstructionStudies\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [00:13<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tMean IoU\tCoeff. of Variation\n",
      "DeepLabV3Plus\t0.9429\t0.0306\n",
      "MaskFormer\t0.9386\t0.0314\n",
      "SegFormer\t0.9359\t0.0402\n",
      "SegNet\t0.9298\t0.0456\n",
      "U-NET\t0.9454\t0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Main evaluation\n",
    "# ======================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "results = []\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "image_dir = os.path.join(data_dir,\"valid\", \"images\")\n",
    "mask_dir = os.path.join(data_dir,\"valid\", \"masks\")\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "dataset = SegEvalDataset(image_dir, mask_dir, image_files)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    model = load_model(model_name, device)\n",
    "    model.eval()\n",
    "\n",
    "    for images, masks, img_names in tqdm(loader):\n",
    "        images = images.to(device)\n",
    "        masks_np = masks.squeeze(1).cpu().numpy()  # [B, H, W]\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                logits = model(images)\n",
    "                preds = (torch.sigmoid(logits) > 0.5).float().cpu().numpy().squeeze(1)  # [B, H, W]\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            mask = masks_np[i]\n",
    "            pred = preds[i]\n",
    "            iou = iou_score(mask, pred)\n",
    "            results.append({\n",
    "                \"modelname\": model_name,\n",
    "                \"IoU\": iou\n",
    "            })\n",
    "\n",
    "# Aggregate results\n",
    "df = pd.DataFrame(results)\n",
    "summary = df.groupby(\"modelname\")[\"IoU\"].agg(['mean', 'std', 'count'])\n",
    "summary['coef_var'] = summary['std'] / (summary['mean'] + 1e-8)\n",
    "\n",
    "# Display results\n",
    "print(\"Model\\tMean IoU\\tCoeff. of Variation\")\n",
    "for idx, row in summary.iterrows():\n",
    "    print(f\"{idx}\\t{row['mean']:.4f}\\t{row['coef_var']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3a0d3",
   "metadata": {},
   "source": [
    "| Model | Mean IoU | Coeff. of Variation |\n",
    "| - | - | - |\n",
    "| DeepLabV3Plus | 0.9429 | 0.0306 |\n",
    "| MaskFormer | 0.9386 | 0.0314 |\n",
    "| SegFormer\t| 0.9359\t| 0.0402\n",
    "| SegNet\t| 0.9298 | 0.0456 |\n",
    "| U-NET\t| 0.9454\t| 0.0309|\n",
    "\n",
    "U-NET is the chosen one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244245c9",
   "metadata": {},
   "source": [
    "## Using the model to get segmented images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977276cf",
   "metadata": {},
   "source": [
    "Segmenting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c6aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gnoceras\\AppData\\Local\\Temp\\ipykernel_26252\\3176427153.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast():\n\u001b[32m     32\u001b[39m         pred_logits = model(image_tensor)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         pred_mask = \u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_logits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.squeeze().numpy()\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Save mask as PNG\u001b[39;00m\n\u001b[32m     36\u001b[39m mask_path = os.path.join(output_folder, filename.replace(\u001b[33m'\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m'\u001b[39m).replace(\u001b[33m'\u001b[39m\u001b[33m.jpeg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m'\u001b[39m).replace(\u001b[33m'\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_mask.png\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.models.uNet import UNet\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Load U-NET model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "channels = [32, 64, 128, 256, 512]\n",
    "model = UNet(in_channels=3, out_channels=1, channels=channels, bilinear=True, use_batchnorm=True).to(device)\n",
    "model.load_state_dict(torch.load(\"../models/U-NET_seg.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "image_folder = \"../images\"\n",
    "output_folder = \"../segmented_images\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_tensor = TF.to_tensor(image_rgb)\n",
    "        image_tensor = TF.normalize(image_tensor, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred_logits = model(image_tensor)\n",
    "                pred_mask = (torch.sigmoid(pred_logits) > 0.5).float().cpu().squeeze().numpy()\n",
    "\n",
    "        # Save mask as PNG\n",
    "        mask_path = os.path.join(output_folder, filename.replace('.jpg', '.png').replace('.jpeg', '.png').replace('.png', '_mask.png'))\n",
    "        cv2.imwrite(mask_path, (pred_mask * 255).astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
